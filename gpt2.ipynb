{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48f33916-7b97-441a-b1dd-22af37807ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f77195-9752-42e1-a51c-8f86292cb6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token:  I\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained model tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions=True)\n",
    "\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Load pre-trained model\n",
    "\n",
    "# Encode some text (replace \"Your text here\" with your input text)\n",
    "input_text = \"My name is Clara and I am?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Predict the next token\n",
    "outputs = model(input_ids)\n",
    "predictions = outputs[0]\n",
    "\n",
    "# Get the predicted next token\n",
    "predicted_index = torch.argmax(predictions[:, -1, :], dim=-1)\n",
    "predicted_token = tokenizer.decode(predicted_index)\n",
    "\n",
    "print(\"Predicted token:\", predicted_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e90fe3-18d5-4766-a18b-52bfcd453bad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443088f9-1740-4537-8be9-1111e4efbd62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Clara and I am a woman. I am a woman who is a woman. I am a woman who is a woman. I am a woman who is a woman. I am a\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_seed = random.randint(0, 10000)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Encode some text (add your own text here)\n",
    "input_text = \"My name is Clara and I am\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Create an attention mask for the inputs\n",
    "attention_mask = torch.ones(input_ids.shape)\n",
    "\n",
    "# Set the model to evaluation mode and disable gradient calculations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Generate predictions\n",
    "    outputs = model.generate(input_ids,\n",
    "                             max_length=40,  # Maximum length of the output text\n",
    "                             num_return_sequences=1,  # Number of sentences to generate\n",
    "                             #temperature=0.5,  # Lower is less random, higher is more random\n",
    "                             # Number of highest probability tokens to keep for top-k filtering\n",
    "                             #attention_mask=attention_mask,  # The attention mask\n",
    "                             pad_token_id=tokenizer.eos_token_id\n",
    "                            )  # Set pad token\n",
    "\n",
    "# Decode and print the output text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f64423c-ceb6-46bb-967f-932237bd91ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/sander/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sander/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load text from a simple-language corpus\n",
    "text = gutenberg.raw('melville-moby_dick.txt') # gutenberg.raw('austen-emma.txt')\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Tokenize text into sentences\n",
    "\n",
    "# Select 1000 random sentences\n",
    "random_sentences = sentences\n",
    "\n",
    "# Simplify each sentence (this is a placeholder - real simplification is complex)\n",
    "# As an example, we split longer sentences into shorter ones\n",
    "simple_sentences = ['.'.join(sent.split('.')[:2]) for sent in random_sentences]\n",
    "\n",
    "model_name = 'gpt2-large'  # Consider using 'gpt2-medium', 'gpt2-large', or 'gpt2-xl'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize and encode sentences in a batch\n",
    "\n",
    "#batch_seq = model.transformer.wte(input_ids['input_ids'][:,:]).float().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cbd55e-1716-473c-8235-932244d20e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9852"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56606dcd-5c3f-490c-b7b0-0ec60dcd7daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_text_list = [''.join([char if char.isalpha() or char.isspace() else '' for char in text]) for text in simple_sentences]\n",
    "\n",
    "input_ids = tokenizer(simple_sentences, return_tensors='pt', padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e10dcdec-133d-4475-a9dd-568c2681ca21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad760a84-3aa5-4e78-ad60-9480ae25324c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9852, 708])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "833851ef-e36c-4f9c-aa21-7ed22eb3fa2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids_flatten = input_ids['input_ids'].flatten()\n",
    "input_ids_flatten = input_ids_flatten[torch.where(input_ids_flatten != input_ids_flatten.max())]\n",
    "\n",
    "#input_ids_flatten = input_ids_flatten.unique(sorted=False)\n",
    "\n",
    "seq_lenght = 5\n",
    "\n",
    "r = input_ids_flatten.shape[0] % seq_lenght\n",
    "\n",
    "input_ids_flatten = input_ids_flatten[:-r]\n",
    "\n",
    "\n",
    "input_ids_to_embed = input_ids_flatten.reshape(-1, seq_lenght)\n",
    "\n",
    "#torch.save(batch_seq, 'tensors/sequences.to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96006c89-45f3-4b5a-a074-4ae151c79a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3bf60b-746b-4be9-aedc-058f9201490d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_seq = model.transformer.wte(input_ids_to_embed[:,:]).float().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f139fa-2dd6-4080-b139-4c85f4c0de51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51ee5095-6c8e-43f2-83a1-a43b5b65bb84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_ar(sequence):\n",
    "    inputs = sequence[:-1]  # All elements except the last\n",
    "    outputs = sequence[1:]  # All elements except the first\n",
    "\n",
    "    A_estimated = torch.linalg.lstsq(inputs, outputs)[0].T\n",
    "    \n",
    "    return A_estimated\n",
    "\n",
    "def generate(A_estimated, s0, n):\n",
    "    # Solve for A\n",
    "    # Multiply with outputs (also transposed) and transpose the result for correct shape\n",
    "\n",
    "    # Initialize the approximated sequence with the first element of the original sequence\n",
    "    approx_sequence = [s0]\n",
    "\n",
    "    # Compute the approximated sequence using the estimated A\n",
    "    for t in range(1, n):\n",
    "        next_element = torch.matmul(A_estimated, approx_sequence[-1].unsqueeze(-1)).squeeze(-1)\n",
    "        approx_sequence.append(next_element)\n",
    "\n",
    "    # Convert the list of tensors to a tensor\n",
    "    approx_sequence = torch.stack(approx_sequence)\n",
    "    return approx_sequence\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689ae823-5635-4cf7-9b2c-a3ac247663af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_seq = model.transformer.wte(input_ids_to_embed[:,:]).float().detach()\n",
    "batch_seq = batch_seq / batch_seq.norm(dim=-1,keepdim=True) #Normalizing each token to unit norm\n",
    "N, T, d = batch_seq.shape\n",
    "\n",
    "tensor_reshaped = batch_seq.reshape(-1, d)\n",
    "\n",
    "# Step 2: Permute rows\n",
    "permuted_indices = torch.randperm(N*T)\n",
    "tensor_permuted = tensor_reshaped[permuted_indices]\n",
    "\n",
    "# Step 3: Reshape back to (N, T, d)\n",
    "rand_batch_seq = tensor_permuted.view(N, T, d)\n",
    "batch_seq = batch_seq[:]\n",
    "rand_batch_seq = rand_batch_seq[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe55de84-2860-4b27-82dc-221edff302ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_seq_tot = batch_seq[:10000] # modify to consider bigger datasets\n",
    "batch_seq = batch_seq_tot[:,:5,:]\n",
    "rand_batch_seq_tot = rand_batch_seq[:10000]\n",
    "rand_batch_seq = rand_batch_seq_tot[:,:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbf376f2-30e2-4059-a7c3-d5ce3ef31d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "approx_sequences = []\n",
    "for i in range(len(batch_seq)):\n",
    "    sequence = batch_seq[i]\n",
    "    A = fit_ar(sequence)\n",
    "    approx_sequence = generate(A, sequence[0], batch_seq_tot.shape[1])\n",
    "    approx_sequences.append(approx_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9f69f0e-903f-4f55-8f53-99ed4be733d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "approx_sequences = torch.stack(approx_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6e0120c-6c84-46b3-8968-550988379eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = (approx_sequences - batch_seq_tot) ** 2\n",
    "\n",
    "rand_approx_sequences = []\n",
    "for i in range(len(rand_batch_seq)):\n",
    "    sequence = rand_batch_seq[i]\n",
    "    A = fit_ar(sequence)\n",
    "    approx_sequence = generate(A, sequence[0], rand_batch_seq_tot.shape[1])\n",
    "    rand_approx_sequences.append(approx_sequence)\n",
    "\n",
    "rand_approx_sequences = torch.stack(rand_approx_sequences)\n",
    "\n",
    "rand_out = (rand_approx_sequences - rand_batch_seq_tot) ** 2\n",
    "\n",
    "out_mean = out.mean((1,2))\n",
    "rand_out_mean = rand_out.mean((1,2))\n",
    "\n",
    "plt.figure(figsize=(2.5, 2.5))\n",
    "\n",
    "plt.hist(out_mean[torch.where(out_mean > 1e-12)], bins=100, color='#007acc', alpha=0.7, label='Original')\n",
    "\n",
    "# Histogram for rand_out_mean with a different nice color\n",
    "plt.hist(rand_out_mean[torch.where(rand_out_mean > 1e-12)], bins=100, color='#cc7000', alpha=0.5, label='Shuffled')\n",
    "\n",
    "# Enhancements\n",
    "plt.xlabel('MSE')  # Label for the x-axis\n",
    "plt.ylabel('Frequency')  # Label for the y-axis\n",
    "plt.legend()  # Add a legend to distinguish the histograms\n",
    "plt.tight_layout()  # Adjust layout to not overlap items\n",
    "plt.grid(axis='y', alpha=0.75)  # Add a grid for better readability, only on y-axis\n",
    "plt.savefig('figures/hist.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86362887-c16e-4db3-8295-af32f944b505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa632a1-195c-40a0-93d0-0f87eaea6dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
